### 决策树的训练
> 决策树的构建过程本质是贪心算法的思路，不是回溯或者考虑全局最优的优化策略。

1. 特征选择：通过某种指标选择最佳分裂特征和分裂点，使子节点的数据“纯度”更高。常用方法：
    - 信息增益（ID3算法）：选择使信息熵下降最多的特征。
    - 增益率（C4.5算法）：改进信息增益，解决对多值特征的偏好问题。
    - 基尼系数（CART分类树）：选择使基尼系数最小的特征。
    - 均方误差（CART回归树）：选择使MSE最小的特征。
2. 节点分裂：递归地将数据按照特征分裂条件划分到子节点，直到满足停止条件（如节点样本数过少、纯度足够高或达到最大深度）。
3. 生成叶子节点
    - 分类树：叶子节点选择多数类作为预测结果。
    - 回归树：叶子节点选择样本均值作为预测结果。

### 损失函数与优化方法
> 决策树的优化目标是通过调整树的结构（分裂特征和分裂点）最小化预测的损失。

1. 损失函数
    - 分类任务：
        - 基尼系数：Gini = $1 - \sum p_i^2$，其中 p_i 是类别 i 的占比。
        - 交叉熵损失：Entropy = $-\sum(p_i * \log p_i)$。
    - 回归任务：
        - 均方误差（MSE）：MSE = $\frac{1}{N}\sum(y_i - y_{pred})^2$。
2. 优化方法
    - 剪枝（Pruning）：
        - 预剪枝：在训练时提前停止分裂（如限制最大深度、最小样本数）。
        - 后剪枝：先生成完整树，再自底向上合并冗余节点（如通过验证集评估剪枝后的泛化性能）。
    - 正则化：
        - 限制树的复杂度，如设置最大深度、最小叶子节点样本数。
        - 使用代价复杂度剪枝（CCP，Cost-Complexity Pruning），平衡模型复杂度和损失。
    - 特征工程：
        - 选择与目标相关性高的特征，避免噪声特征干扰分裂。
        - 对连续特征进行分箱（Binning）或离散化处理。
3. 集成学习（进阶优化），单棵决策树容易过拟合，可通过集成方法提升性能：
    - 随机森林（Random Forest）：多棵树的投票结果。
    - 梯度提升树（GBDT）：通过梯度下降迭代优化多棵树的加权预测结果。

### Python代码实现分类树
```python
import numpy as np
from collections import Counter

# 定义决策树节点类
class TreeNode:
    def __init__(self, feature_idx=None, threshold=None, left=None, right=None, value=None):
        self.feature_idx = feature_idx  # 分裂特征索引
        self.threshold = threshold      # 分裂阈值（用于连续特征）
        self.left = left                # 左子树（<=阈值）
        self.right = right              # 右子树（>阈值）
        self.value = value              # 叶子节点的类别（非叶子节点为None）

# 决策树分类模型
class DecisionTreeClassifier:
    def __init__(self, max_depth=3, min_samples_split=2):
        self.max_depth = max_depth          # 最大深度
        self.min_samples_split = min_samples_split  # 节点最小样本数（小于此值不分裂）
        self.root = None

    def fit(self, X, y):
        self.root = self._grow_tree(X, y, depth=0)

    # 递归构建树
    def _grow_tree(self, X, y, depth):
        n_samples, n_features = X.shape
        n_classes = len(np.unique(y))

        # 终止条件：达到最大深度或样本数不足或纯度足够
        if (depth >= self.max_depth or 
            n_samples < self.min_samples_split or 
            n_classes == 1):
            return TreeNode(value=self._most_common_label(y))

        # 寻找最佳分裂特征和阈值
        best_feature, best_threshold = self._best_split(X, y)

        # 如果无法分裂（如所有特征相同），返回叶子节点
        if best_feature is None:
            return TreeNode(value=self._most_common_label(y))

        # 根据最佳分裂点分割数据
        left_idxs = X[:, best_feature] <= best_threshold
        right_idxs = X[:, best_feature] > best_threshold
        left = self._grow_tree(X[left_idxs], y[left_idxs], depth+1)
        right = self._grow_tree(X[right_idxs], y[right_idxs], depth+1)

        return TreeNode(feature_idx=best_feature, threshold=best_threshold, left=left, right=right)

    # 计算基尼系数
    def _gini(self, y):
        counts = np.bincount(y)
        p = counts / len(y)
        return 1 - np.sum(p ** 2)

    # 寻找最佳分裂点
    def _best_split(self, X, y):
        best_gini = float('inf')
        best_feature, best_threshold = None, None

        for feature_idx in range(X.shape[1]):
            thresholds = np.unique(X[:, feature_idx])
            for threshold in thresholds:
                # 按阈值分割
                left_idxs = X[:, feature_idx] <= threshold
                right_idxs = X[:, feature_idx] > threshold

                if len(y[left_idxs]) == 0 or len(y[right_idxs]) == 0:
                    continue

                # 计算分裂后的加权基尼系数
                gini_left = self._gini(y[left_idxs])
                gini_right = self._gini(y[right_idxs])
                weighted_gini = (len(y[left_idxs]) * gini_left + 
                                 len(y[right_idxs]) * gini_right) / len(y)

                # 更新最佳分裂点
                if weighted_gini < best_gini:
                    best_gini = weighted_gini
                    best_feature = feature_idx
                    best_threshold = threshold

        return best_feature, best_threshold

    # 获取多数类别标签
    def _most_common_label(self, y):
        counter = Counter(y)
        return counter.most_common(1)[0][0]

    # 预测单个样本
    def _predict(self, x, node):
        if node.value is not None:
            return node.value
        if x[node.feature_idx] <= node.threshold:
            return self._predict(x, node.left)
        else:
            return self._predict(x, node.right)

    # 批量预测
    def predict(self, X):
        return np.array([self._predict(x, self.root) for x in X])

# 示例：鸢尾花数据集分类
if __name__ == "__main__":
    from sklearn.datasets import load_iris
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score

    # 加载数据
    iris = load_iris()
    X, y = iris.data, iris.target

    # 划分训练集和测试集
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # 训练模型
    tree = DecisionTreeClassifier(max_depth=3)
    tree.fit(X_train, y_train)

    # 预测并评估
    y_pred = tree.predict(X_test)
    print(f"Test Accuracy: {accuracy_score(y_test, y_pred):.4f}")
```